# 阿毗达摩中文模型 LLM-abhidharma 

## 依赖项 Requirements

- 数据处理部分无依赖项
- 训练部分参考 [ymcui/Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) 

## 训练流程 Procedure

1. 数据处理
    1. 在 CBETA 下载《俱舍论》《顺正理论》《大毗婆沙论》，包含现代标点（婆沙论只有句号标记的断句）
    2. 使用 opencc 将繁体汉字转换为简体（`script/convert_zh.py`）
    3. 根据标点符号和关键词将文本转换为问答对（`script/txt2json*.py`）
        - 《俱舍》《顺正理论》包含两种问答：
            1. 以颂为问题，以论为回答
            2. 以论中的问号分割问题和答案
        - 《婆沙》按段落扫描，按照“问”字分段
            - 如果有“答”字，就以此分割问题和答案
            - 如果没有，就以第一个句读分割问题和答案
2. 指令微调

## 训练数据 Data

- `zh-CN/`: 简体汉字文本
- `JSON/`: 问答对
- `wanglu.json`: 王路老师的研究内容，作为验证集，暂未加入训练集

### 关于数据的一些观察 Some observations

- 模型在形式上学习得很好，比如句末括号里煞有介事地给出来源（编的），又比如开头的“论曰”。
    - 其中如果原文的颂按照训练集的格式提问，回答（也是编的）
    - 拿其他内容
- 训练时选定的窗口为 512，颂-论的长度远大于此，论内部的问答远小于此，这导致大量数据包含结束符，所以模型输出非常短；同时长问答可能被截断，后半截没有学到
- 《婆沙》的内容比《俱舍》和《正理》多很多，这导致模型更容易编造说某句话来自婆沙。可能需要平衡数据集的权重，这我在 torch.util.DataLoader 里会做，但是 transformer 里怎么做还需要学。
- 训练数据都是文言文，提问用白话文，可能是效果不佳的原因。打算使用对文言文专攻的模型将其翻译一遍，但担心出现理解错误。

## 许可协议 LICENCE

训练数据遵循 CBETA 相同协议流通，详见《[财团法人佛教电子佛典基金会资料库版权宣告](https://www.cbeta.org/copyright.php)》
